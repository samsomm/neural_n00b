{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN stock prices example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"One to One\" RNN architecture\n",
    "\n",
    "- In a \"One to One\" RNN architecture, the model takes one input and produces one output.\n",
    "\n",
    "- In this case, we're using the stock prices of two consecutive days (yesterday and today) as input to predict the stock price for the next day (tomorrow). \n",
    "\n",
    "- This is still considered a \"One to One\" architecture because for each input sequence (consisting of two days' prices), the RNN produces a single output (the price for the next day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation\n",
    "# Days array remains the same\n",
    "dataset_len = 100\n",
    "\n",
    "x = np.array(range(1, dataset_len))  # Days\n",
    "\n",
    "# Fixed stock prices array in a simple descending order\n",
    "#y = np.array(range(20, 0, -1))  # Stock prices\n",
    "y = np.sin(x * np.pi / 18)\n",
    "\n",
    "print(\"Days array\", x)\n",
    "print(\"Stock prices array\", y)\n",
    "\n",
    "def ploting_the_data(x, y):\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, marker='o')\n",
    "    plt.title('Stock Price Trend Over 20 Days')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Stock Price (sin function)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "ploting_the_data(x, y)\n",
    "\n",
    "# Convert the list of sequences to a NumPy array before converting to a tensor\n",
    "# prepare time frames for 2 days yestarday and today - array([17, 16]) \n",
    "\n",
    "# Hyper Parameter - Time frame window size for predictions\n",
    "time_frame_window_size = 3\n",
    "\n",
    "X_np = np.array([y[n-time_frame_window_size:n] for n in range(time_frame_window_size, dataset_len)])\n",
    "X = torch.tensor(X_np, dtype=torch.float).unsqueeze(-1)\n",
    "Y = torch.tensor(y[time_frame_window_size:], dtype=torch.float)\n",
    "\n",
    "\n",
    "print(\"X.shape\", X.shape)\n",
    "print(\"Y.shape\", Y.shape)\n",
    "\n",
    "print(\"X[0]\", X[0])\n",
    "print(\"X[1]\", X[1])\n",
    "print(\"Y[1]\", Y[0])\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_size = time_frame_window_size  # Because we are inputting N day's price at a time\n",
    "hidden_size = 2  # Size of the RNN's hidden state\n",
    "output_size = 1  # We want to output one price\n",
    "\n",
    "# Define the RNN model\n",
    "class StockRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(StockRNN, self).__init__()\n",
    "        # RNN layer: Defines a simple RNN layer with the specified input and hidden size.\n",
    "        # 'batch_first=True' indicates that the first dimension of the input and output will be the batch size.        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)  # RNN layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer for output DNN \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x.shape => \", x.shape)\n",
    "        x = x.squeeze(-1)\n",
    "        #print(\"x 2D=> \", x.shape) \n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        res = self.fc(out)  # Final output for each sequence\n",
    "        #print(\"res \", res.shape) \n",
    "        return res\n",
    "\n",
    "# Instantiate the model\n",
    "model = StockRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(220):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)  # Forward pass\n",
    "    loss = criterion(output, Y)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/200], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Predict the next day's price\n",
    "\n",
    "last_working_week = X[-1].T\n",
    "print(\"last week shape => \", last_working_week.shape)\n",
    "\n",
    "# update price by shifting window +1 step and add predicted price for NEXT day.\n",
    "for i in range(time_frame_window_size):\n",
    "    print(\"last_working_week => \", last_working_week)    \n",
    "    predicted_price = model(last_working_week)\n",
    "    print(f\"Predicted price for tomorrow: {predicted_price.item():.2f}\")\n",
    "    \n",
    "    x = np.append(x, len(x)+1)\n",
    "    y = np.append(y, predicted_price.detach().numpy()[0])\n",
    "    \n",
    "    print(\"Updated X => \", x)\n",
    "    print(\"Updated Y => \", y)\n",
    "    \n",
    "    last_working_week = torch.tensor(torch.from_numpy(x[-time_frame_window_size:]), dtype=torch.float)\n",
    "    last_working_week = last_working_week.reshape(1,time_frame_window_size)\n",
    "    \n",
    "ploting_the_data(x, y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Many to Many\" RNN architecture \n",
    "\n",
    "- \"One to Many\" RNN, the model takes one input and produces a sequence of outputs. \n",
    "\n",
    "- For this case, let's say the model takes the stock price of one day and predicts the prices for the next few days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM from scratch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/imax/Documents/github/neural_n00b/env_nn_n00b/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 12    \n",
      "--------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_remember_percent =>  tensor(0.5000, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5000, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0., grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0., grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5000, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3898, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4296, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4396, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1888, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4461, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4389, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4736, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3225, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2356, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4620, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2842, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3726, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7895, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3611, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3801, grad_fn=<SigmoidBackward0>)\n",
      "Company A: Observed = 0, Predicted = tensor(0.1316)\n",
      "long_remember_percent =>  tensor(0.2899, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3619, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7368, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2666, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3935, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3836, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4404, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5321, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3366, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4328, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4353, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4797, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3817, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3297, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4545, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2823, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3764, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8057, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3963, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3754, grad_fn=<SigmoidBackward0>)\n",
      "Company A: Observed = 1, Predicted = tensor(0.1415)\n",
      "long_remember_percent =>  tensor(0.9888, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9446, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-0.9444, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8969, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9886, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9440, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-1.8775, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8961, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9884, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9434, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-2.7990, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8952, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9882, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9428, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-3.7085, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8944, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9880, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9422, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-4.6059, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8936, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9878, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9416, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-5.4909, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8928, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9875, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9409, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-6.3632, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8920, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9873, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9403, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-7.2226, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8911, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9871, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9397, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-8.0688, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8903, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9869, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9390, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-8.9016, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8894, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9866, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9384, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-9.7207, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8886, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9864, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9377, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9998, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-10.5259, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8877, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9861, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9370, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-11.3168, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8868, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9859, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9364, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-12.0934, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8860, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9856, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9357, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-12.8552, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8851, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9854, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9350, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-13.6022, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8842, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9851, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9343, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-14.3339, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8833, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9849, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9336, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-15.0503, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8824, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9846, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9329, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-15.7511, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8815, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9843, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9322, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-16.4361, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8806, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9840, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9315, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9997, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-17.1050, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8797, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9838, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9307, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-17.7576, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8788, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9835, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9300, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-18.3937, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8779, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9832, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9292, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-19.0132, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8769, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9829, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9285, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-19.6158, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8760, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9826, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9277, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-20.2014, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8751, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9823, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9270, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-20.7697, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8741, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9820, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9262, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9996, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-21.3206, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8732, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9816, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9254, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9995, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-21.8539, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8722, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9813, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9246, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9995, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-22.3696, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8712, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9810, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9238, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9995, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-22.8673, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8703, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9806, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9230, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9995, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-23.3471, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8693, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9803, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9222, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9995, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-23.8087, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8683, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9799, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9214, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9994, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-24.2520, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8673, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9796, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9206, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9994, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-24.6770, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8663, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9792, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9198, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9994, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.0836, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8653, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9789, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9189, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9994, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.4716, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8643, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9785, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9181, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9994, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.8411, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8633, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9781, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9172, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9993, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.1918, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8622, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9777, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9163, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9993, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.5239, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8612, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9773, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9155, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9993, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.8372, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8602, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9769, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9146, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9992, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.1317, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8591, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9765, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9137, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9992, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.4075, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8581, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9761, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9128, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9992, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.6645, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8570, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9757, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9119, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9992, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.9028, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8560, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9752, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9110, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9991, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.1224, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8549, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9748, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9101, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9991, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.3233, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8538, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9744, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9091, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9991, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.5056, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8527, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9739, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9082, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9990, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.6694, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8516, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9735, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9072, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9990, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.8148, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8505, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9730, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9063, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9989, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.9419, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8494, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9725, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9053, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9989, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.0508, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8483, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9720, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9043, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9989, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.1416, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8472, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9715, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9033, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9988, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.2146, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8461, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9710, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9023, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9988, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.2698, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8450, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9705, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9013, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9987, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.3076, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8438, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9700, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.9003, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9987, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.3280, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8427, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9695, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8993, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9986, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.3313, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8415, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9690, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8983, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9986, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.3178, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8404, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9684, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8972, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9985, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.2877, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8392, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9679, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8962, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9985, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.2412, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8380, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9673, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8951, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9984, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.1787, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8369, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9667, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8940, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9983, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.1004, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8357, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9661, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8930, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9983, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-29.0066, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8345, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9656, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8919, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9982, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.8978, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8333, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9650, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8908, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9981, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.7742, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8321, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9643, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8897, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9981, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.6361, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8309, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9637, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8886, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9980, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.4840, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8296, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9631, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8874, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9979, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.3182, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8284, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9624, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8863, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9978, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-28.1392, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8272, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9618, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8851, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9978, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.9472, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8259, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9611, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8840, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9977, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.7429, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8247, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9605, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8828, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9976, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.5264, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8234, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9598, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8816, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9975, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.2984, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8222, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9591, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8804, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9974, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-27.0593, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8209, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9584, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8792, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9973, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.8094, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8196, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9576, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8780, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9972, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.5494, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8184, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9569, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8768, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9971, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.2796, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8171, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9562, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8756, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9970, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-26.0005, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8158, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9554, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8743, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9969, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.7126, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8145, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9546, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8731, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9967, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.4164, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8132, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9539, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8718, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9966, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-25.1123, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8118, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9531, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8706, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9965, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-24.8010, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8105, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9522, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8693, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9963, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-24.4828, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8092, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9514, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8680, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9962, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-24.1583, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8078, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9506, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8667, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9961, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-23.8280, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8065, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9497, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8654, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9959, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-23.4923, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8051, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9489, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8640, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9957, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-23.1518, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8038, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9480, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8627, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9956, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-22.8069, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8024, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9471, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8613, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9954, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-22.4582, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.8010, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9462, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8600, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9952, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-22.1060, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7997, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9453, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8586, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9951, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-21.7510, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7983, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9444, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8572, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9949, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-21.3935, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7969, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9434, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8558, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9947, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-21.0341, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7955, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9424, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8544, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9945, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-20.6731, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7941, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9415, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8530, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9943, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-20.3111, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7926, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9405, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8516, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9940, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-19.9484, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7912, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9395, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8501, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9938, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-19.5856, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7898, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9384, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8487, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9936, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-19.2229, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7883, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.9374, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8472, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.9933, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-18.8608, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.7869, grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imax/Documents/github/neural_n00b/env_nn_n00b/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/imax/Documents/github/neural_n00b/env_nn_n00b/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71845bd6431a4064b1002000989259d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_remember_percent =>  tensor(0.5000, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5000, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0., device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0., device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5000, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3898, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4296, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4396, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1888, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4461, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4389, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4736, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3225, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2356, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4620, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2842, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3726, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7895, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3611, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3801, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2894, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3614, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7359, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2660, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3930, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3832, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4400, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5306, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3354, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4324, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4350, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4793, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3800, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3280, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4542, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2819, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3758, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8046, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3949, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3750, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4999, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4999, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(-0.0004, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(-0.0002, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4999, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3897, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4294, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4390, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1885, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4460, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4389, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4734, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3218, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2351, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4619, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2841, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3724, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7890, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3606, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3799, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2899, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3619, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7368, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2666, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3935, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3836, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4405, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5321, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3367, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4328, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4354, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4797, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3817, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3297, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4545, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2823, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3764, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8057, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3964, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3754, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5002, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5002, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0005, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0003, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5002, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3901, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4298, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4404, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1894, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4464, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4392, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4738, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3235, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2365, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4622, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2846, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3730, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7901, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3620, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3804, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2904, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3624, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7378, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2674, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3941, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3841, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4410, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5337, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3380, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4332, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4357, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4802, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3836, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3315, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4548, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2828, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3771, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8069, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3980, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3759, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5005, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5005, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0016, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0008, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5005, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3905, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4303, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4419, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1905, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4468, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4396, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4743, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3253, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2380, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4625, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2850, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3736, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7913, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3635, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3809, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2909, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3630, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7388, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2681, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3946, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3845, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4415, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5353, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3394, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4336, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4361, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4807, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3856, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3334, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4551, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2833, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3778, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8080, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3997, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3764, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5008, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5007, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0026, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0013, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5008, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3909, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4307, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4435, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1915, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4472, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4399, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4747, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3272, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2396, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4628, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2855, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3743, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7925, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3650, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3813, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2914, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3635, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7398, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2689, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3952, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3850, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4420, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5369, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3408, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4340, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4365, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4812, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3876, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3353, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4555, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2838, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3785, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8092, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4014, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3769, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5011, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5010, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0037, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0019, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5010, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3914, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4312, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4451, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1926, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4476, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4403, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4752, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3291, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2412, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4632, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2860, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3749, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7937, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3666, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3818, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2920, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3640, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7408, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2697, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3958, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3854, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4425, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5385, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3422, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4344, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4369, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4817, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3896, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3372, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4558, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2843, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3792, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8104, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4031, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3773, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5014, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5013, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0048, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0024, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5013, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3918, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4317, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4467, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1938, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4480, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4407, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4756, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3310, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2428, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4635, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2865, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3756, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7949, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3682, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3823, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2925, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3646, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7418, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2705, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3964, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3859, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4430, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5402, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3437, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4348, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4373, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4821, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3916, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3391, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4561, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2848, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3799, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8116, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4049, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3778, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5018, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5016, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0059, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5016, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3922, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4321, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4483, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1949, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4484, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4411, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4761, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3329, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2444, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4638, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2870, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3763, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7962, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3697, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3828, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2930, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3652, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7428, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2712, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3969, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3863, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4435, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5418, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3451, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4352, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4377, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4826, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3937, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3411, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4564, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2853, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3806, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8128, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4066, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3783, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5021, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5019, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0070, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0035, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5019, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3927, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4326, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4499, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1960, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4488, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4415, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4765, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3348, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2461, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4641, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2875, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3769, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7974, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3713, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3833, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2935, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3657, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7438, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2720, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3975, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3868, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4441, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5435, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3466, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4357, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4381, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4831, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3957, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3430, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4568, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2858, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3813, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8140, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4084, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3788, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5024, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5022, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0081, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0041, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5022, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3931, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4331, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4515, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1971, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4492, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4418, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4770, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3368, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2477, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4645, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2881, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3776, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7986, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3729, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3838, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2941, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3663, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7448, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2728, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3981, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3872, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4446, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5451, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3480, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4361, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4385, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4836, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3977, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3450, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4571, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2863, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3820, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8152, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4102, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3794, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5027, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5025, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0092, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0046, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5025, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3936, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4335, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4531, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1983, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4496, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4422, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4775, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3387, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2494, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4648, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2886, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3783, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7998, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3745, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3844, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2946, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3668, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7458, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2736, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3987, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3877, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4451, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5468, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3495, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4365, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4389, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4841, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3998, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3469, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4574, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2869, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3827, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8164, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4119, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3799, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5030, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5028, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0103, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0052, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5029, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3940, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4340, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4547, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.1994, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4500, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4426, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4779, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3406, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2510, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4651, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2891, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3790, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8010, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3762, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3849, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2951, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3674, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7468, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2744, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3992, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3882, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4456, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5484, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3509, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4369, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4393, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4846, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4018, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3489, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4578, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2874, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3834, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8176, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4137, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3804, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5034, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5031, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0114, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0057, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5032, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3945, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4345, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4563, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2005, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4505, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4430, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4784, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3425, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2527, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4655, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2896, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3797, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8022, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3778, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3854, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2956, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3679, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7478, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2752, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3998, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3886, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4462, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5501, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3524, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4374, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4397, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4851, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4038, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3508, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4581, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2879, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3841, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8187, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4155, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3809, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5037, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5034, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0125, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0063, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5035, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3949, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4350, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4579, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2016, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4509, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4434, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4789, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3445, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2544, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4658, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2901, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3804, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8034, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3794, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3859, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2962, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3685, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7488, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2759, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4004, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3891, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4467, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5517, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3538, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4378, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4401, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4856, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4058, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3528, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4584, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2884, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3848, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8199, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4173, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3814, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5040, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5037, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0135, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0068, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5038, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3953, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4354, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4595, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2028, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4513, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4438, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4793, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3464, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2560, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4661, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2906, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3810, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8046, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3810, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3864, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2967, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3691, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7498, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2767, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4010, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3896, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4472, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5533, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3553, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4382, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4405, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4861, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4079, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3548, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4588, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2890, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3855, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8211, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4190, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3819, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5043, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5040, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0146, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0074, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5041, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3958, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4359, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4610, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2039, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4517, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4442, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4798, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3483, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2577, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4665, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2912, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3817, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8058, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3826, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3869, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2973, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3696, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7508, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2775, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4016, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3900, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4478, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5549, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3567, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4386, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4409, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4866, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4099, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3567, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4591, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2895, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3862, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8222, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4208, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3824, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5047, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5043, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0157, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0079, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5044, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3962, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4364, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4626, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2050, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4521, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4446, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4803, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3502, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2593, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4668, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2917, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3824, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8070, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3843, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3874, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2978, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3702, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7518, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2783, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4022, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3905, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4483, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5566, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3582, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4391, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4413, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4871, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4119, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3587, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4594, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2900, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3869, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8233, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4226, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3829, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5050, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5046, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0168, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0085, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5047, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3967, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4369, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4642, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2062, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4525, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4450, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4807, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3521, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2610, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4671, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2922, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3831, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8081, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3859, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3879, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2983, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3707, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7527, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2791, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4027, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3910, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4488, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5582, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3596, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4395, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4417, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4876, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4139, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3607, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4598, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2905, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3877, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8245, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4244, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3834, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5053, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5049, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0179, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0090, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5050, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3971, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4373, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4658, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2073, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4530, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4454, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4812, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3540, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2627, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4675, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2927, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3838, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8093, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3875, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3885, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2989, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3713, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7537, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2798, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4033, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3914, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4493, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5598, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3611, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4399, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4421, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4882, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4159, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3626, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4601, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2911, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3884, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8256, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4262, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3839, device='mps:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_remember_percent =>  tensor(0.5056, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5052, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0190, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0096, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5053, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3976, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4378, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4673, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2084, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4534, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4458, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4817, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3559, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2643, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4678, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2933, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3845, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8105, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3891, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3890, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2994, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3719, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7546, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2806, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4039, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3919, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4499, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5614, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3625, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4404, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4425, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4887, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4179, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3646, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4604, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2916, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3891, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8267, device='mps:0', grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4280, device='mps:0', grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3845, device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.5059, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5055, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.0200, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.0101, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.5056, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3980, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4383, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4689, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2095, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4538, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4462, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4821, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3578, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2660, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4681, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2938, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3852, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8116, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3908, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3895, grad_fn=<SigmoidBackward0>)\n",
      "Company A: Observed = 0, Predicted = tensor(0.1449)\n",
      "long_remember_percent =>  tensor(0.2997, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3722, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.7551, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.2810, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4042, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.3922, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4502, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.5622, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3633, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4406, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.4428, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4889, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.4189, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.3657, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.4606, grad_fn=<SigmoidBackward0>)\n",
      "long_remember_percent =>  tensor(0.2919, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.3895, grad_fn=<SigmoidBackward0>)\n",
      "potential_remember_percent =>  tensor(0.8273, grad_fn=<TanhBackward0>)\n",
      "updated_long_memory =>  tensor(0.4290, grad_fn=<AddBackward0>)\n",
      "output_percent =>  tensor(0.3847, grad_fn=<SigmoidBackward0>)\n",
      "Company A: Observed = 1, Predicted = tensor(0.1556)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pytorch_lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class LSTMFromScrutch(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LSTMFromScrutch, self).__init__()\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "        \n",
    "        # Initialize weights and biases for the Forget Gate        \n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad = True)\n",
    "        \n",
    "        # Initialize weights and biases for the Input Gate\n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad = True)\n",
    "        \n",
    "        # Initialize weights and biases for the Tanh function to create new memory\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad = True)\n",
    "        \n",
    "        # Initialize weights and biases for the Output Gate\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad = True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad = True)\n",
    "        \n",
    "        #for the visualisation gates: \n",
    "        self\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        # Calculate the forget gate activation\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) +\n",
    "                                              (input_value * self.wlr2) +\n",
    "                                              self.blr1)\n",
    "\n",
    "        print(\"long_remember_percent => \", long_remember_percent)\n",
    "        # Calculate the input gate activation\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) + \n",
    "                                                  (input_value * self.wpr2) +\n",
    "                                                  self.bpr1)\n",
    "       \n",
    "        print(\"potential_remember_percent => \", potential_remember_percent)  \n",
    "        # Calculate the new candidate values\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + \n",
    "                                      (input_value * self.wp2) +\n",
    "                                      self.bp1)\n",
    "        \n",
    "        print(\"potential_remember_percent => \", potential_memory)  \n",
    "        # Update the long-term memory\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + \n",
    "                              (potential_remember_percent * potential_memory))\n",
    "       \n",
    "        print(\"updated_long_memory => \", updated_long_memory)   \n",
    "        # Calculate the output gate activation\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) +\n",
    "                                       (input_value * self.wo2) +\n",
    "                                       self.bo1)\n",
    "        \n",
    "        print(\"output_percent => \", output_percent)   \n",
    "        \n",
    "        # Update the short-term memory\n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "        \n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Initialize long-term and short-term memories\n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "        # Process each day's input through the LSTM unit\n",
    "        for day in input:\n",
    "            long_memory, short_memory = self.lstm_unit(day, long_memory, short_memory)\n",
    "        \n",
    "        # Return the final short-term memory as the output\n",
    "        return short_memory\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Configure the optimizer for training\n",
    "        return Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Perform a single training step\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i)**2\n",
    "        # pytorch_lightning does not use explicit .backward() call:\n",
    "        # loss.backward()  # Backpropagation\n",
    "        \n",
    "        # Log the training loss and output based on the label\n",
    "        self.log(\"training_loss\", loss)\n",
    "        if (label_i==0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "model = LSTMFromScrutch()\n",
    "\n",
    "# Predict without training \n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company A: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())\n",
    "\n",
    "\n",
    "# Applying the ReLU function to the value 2\n",
    "#result = relu(2)\n",
    "\n",
    "\n",
    "#Vanishing and Exploding Gradient solving\n",
    "long_memory = 0\n",
    "short_memory = 0\n",
    "\n",
    "x0 = -5\n",
    "x_step = 0.02\n",
    "for i in range(100):\n",
    "    step = x0 + i*x_step \n",
    "    updated_long_memory, updated_short_memory = model.lstm_unit(step, long_memory, short_memory)\n",
    "    long_memory = updated_long_memory\n",
    "    short_memory = short_memory \n",
    "\n",
    "\n",
    "# Prepare data for training \n",
    "inputs = torch.tensor([\n",
    "    [0., 0.5, 0.25, 1.],\n",
    "    [1., 0.5, 0.25, 1.]\n",
    "])\n",
    "labels = torch.tensor([0.,1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, num_workers=4)\n",
    "\n",
    "# Train \n",
    "trainer = L.Trainer(max_epochs=20)\n",
    "trainer.fit(model, train_dataloaders = dataloader)\n",
    "\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company A: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())\n",
    "\n",
    "\n",
    "# path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "# trainer = L.Trainer(max_epochs=3000)\n",
    "# trainer.fit(model, train_dataloaders = dataloader, ckpt_path = path_to_best_checkpoint)\n",
    "\n",
    "# print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "# print(\"Company A: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())\n",
    "\n",
    "# path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "# trainer = L.Trainer(max_epochs=5000)\n",
    "# trainer.fit(model, train_dataloaders = dataloader, ckpt_path = path_to_best_checkpoint)\n",
    "\n",
    "# print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "# print(\"Company A: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM from torch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM torch implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "class LightningLSTM(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LightningLSTM, self).__init__()\n",
    "        \n",
    "        # Input Preparation: Your input X is a single value (0 in this case). \n",
    "        # Since the input size of your LSTM is set to input_size=1, \n",
    "        # this means your network is configured to process one feature at a time.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=3, batch_first=True)\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        linear_out = self.linear(last_time_step_out)\n",
    "        prediction = self.sigmoid(linear_out)\n",
    "        return prediction\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_function(outputs, labels.unsqueeze(1))\n",
    "        # pytorch_lightning does not use explicit .backward() call:\n",
    "        # loss.backward()  # Backpropagation\n",
    "        self.log(\"training_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "# Example usage (Dummy data for demonstration)\n",
    "inputs = torch.tensor([\n",
    "    [0., 0.5, 0.25, 1.],\n",
    "    [1., 0.5, 0.25, 1.]\n",
    "]).unsqueeze(-1)\n",
    "\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1, num_workers=4)\n",
    "\n",
    "model = LightningLSTM()\n",
    "trainer = pl.Trainer(max_epochs=2, log_every_n_steps=2)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "# Test predictions\n",
    "with torch.no_grad():\n",
    "    print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([[0., 0.5, 0.25, 1.]]).unsqueeze(-1)))\n",
    "    print(\"Company A: Observed = 1, Predicted =\", model(torch.tensor([[1., 0.5, 0.25, 1.]]).unsqueeze(-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs)\n",
    "\n",
    "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture that was introduced to solve some of the limitations of traditional RNNs, particularly the problem of long-term dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nn_n00b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
