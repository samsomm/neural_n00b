{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### TODO Concepts of RNN / LSTM /GRU and also working of LSTM\n",
    "\n",
    "#### The Encoder-Decoder is a neural network discovered in 2014. It is a fundamental pillar of Deep Learning.\n",
    "\n",
    "Encoder-decoder architectures are foundational to a variety of applications in deep learning, especially in natural language processing and computer vision. They are commonly used for tasks such as image captioning, machine translation, and text summarization, data compression, or as a pre-processing step for more complex tasks like machine translation.\n",
    "\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "These two neural networks usually have the same structure.\n",
    "The first one will be used normally but the second one will work in reverse:\n",
    "\n",
    "- **A first neural network** will take a sentence as input to output a sequence of numbers.\n",
    "\n",
    "- **The second network** will take this sequence of numbers as input to output a sentence this time!\n",
    "\n",
    "In fact these two networks do the same thing. But one is used in the normal direction and the other in the **opposite.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[img/encoder_decoder.png]\n",
    "\n",
    "- Encoder-It accepts a single element of the input sequence at each time step, process it, collects information for that element and propagates it forward.\n",
    "\n",
    "- Intermediate vector- This is the final internal state produced from the encoder part of the model. It contains information about the entire input sequence to help the decoder make accurate predictions.\n",
    "\n",
    "- Decoder- given the entire sentence, it predicts an output at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer is 0, amount of teurons is 784\n",
      "layer is 1, amount of teurons is 200\n",
      "layer is 2, amount of teurons is 100\n",
      "layer is 3, amount of teurons is 50\n",
      "layer is 4, amount of teurons is 50\n",
      "layer is 5, amount of teurons is 100\n",
      "layer is 6, amount of teurons is 200\n",
      "layer is 7, amount of teurons is 784\n"
     ]
    }
   ],
   "source": [
    "from src.autoencoder_n00b import Autoencoder\n",
    "from src.dataset_service import read_mnist_data\n",
    "\n",
    "#\n",
    "# Initiate   \n",
    "#\n",
    "layers = [784, 200, 100, 50]\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "autoencoder.create_autoencoder(layers)\n",
    "autoencoder.show_autoencoder_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. pay ATTENTION for autoencoder target == input. \n",
    "# In other words NN have to restore initial data from the layers with less neurons than input layer\n",
    "# autoencoder.train(inputs_list=train_inputs, targets_list=train_inputs, learning_rate=0.1, epoch=5)\n",
    "train_path = \"./data/mnist_train_60k.csv.zip\"\n",
    "train_inputs, train_targets = read_mnist_data(output_nodes_amount=10, samples=1000, csv_path=train_path)\n",
    "\n",
    "autoencoder.train(inputs_list=train_inputs, targets_list=train_inputs, learning_rate=0.1, epoch=1)\n",
    "\n",
    "\n",
    "#res = nn.pred(train_inputs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder / Decoder for language translation explained\n",
    "\n",
    "So we have a sentence, a sequence of words, which is encoded into a sequence of numbers, then decoded into another sequence of words : the translated sentence.\n",
    "\n",
    "The first neural network is called the encoder and the second neural network the decoder.\n",
    "\n",
    "For example, if we have the sentence “prendre une expression au pied de la lettre” and we translate it word by word, we would obtain: “take an expression at the foot of the letter”. It doesn’t mean anything in English.\n",
    "\n",
    "But the Encoder Decoder approach solves this problem.\n",
    "\n",
    "Indeed, the structure of the encoder allows to extract the meaning of a sentence.\n",
    "\n",
    "It stores the extracted information in a vector (the result of the encoder).\n",
    "\n",
    "Then, the decoder analyzes the vector to produce its own version of the sentence.\n",
    "\n",
    "In our previous example, the Encoder-Decoder would be closer to the actual translation: “take an expression literally”.\n",
    "\n",
    "This result is much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO text to code generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are used to convert discrete tokens (such as words) into continuous vectors in a high-dimensional space. These vectors capture the semantic information and relationships between tokens, allowing models to understand and process them more effectively.\n",
    "\n",
    "#### When do we use an encoder decoder model?\n",
    "\n",
    "- 1-Image Captioning. Encoder decoder models allow for a process in which a machine learning model generates a sentence describing an image. It receives the image as the input and outputs a sequence of words. This also works with videos.\n",
    "\n",
    "- 2-Sentiment Analysis. These models understand the meaning and emotions of the input sentence and output a sentiment score. It is usually rated between -1 (negative) and 1 (positive) where 0 is neutral. It is used in call centers to analyse the evolution of the client’s emotions and their reactions to certain keywords or company discounts.\n",
    "\n",
    "- 3-Translation. This model reads an input sentence, understands the message and the concepts, then translates it into a second language. Google Translate is built upon an encoder decoder structure.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
